{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b09dd89",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook shows how to run Stage3 pipeline version 1.12.5 (Release 20.10.2023). Previous JWST pipelines realeses can be check here: https://github.com/spacetelescope/jwst/releases \n",
    "\n",
    "## 1.1 JWST PIPELINE INSTALLATION<a name=\"pipeline_installation\"></a>\n",
    "In this section I explain how to install the JWST pipeline, necesary for the data reduction. \n",
    "\n",
    "The easiest way to install the pipeline is via pip. Below we show how to create a new conda environment, activate that environment, and then install the latest released version of the pipeline. You can name your environment anything you like. In the lines below, replace < env_name > with your chosen environment name.\n",
    "\n",
    "<code> conda create -n <env_name> python\n",
    "<code> conda activate <env_name>\n",
    "    \n",
    "After Creating the conda enviroment, you need to add the CDRS files to the bash_profile. \n",
    "you can simply use this two code lines in your terminal;\n",
    "    \n",
    "<code> touch ~/.bash_profile; open ~/.bash_profile </code>\n",
    "    \n",
    "And inside the TEXT file paste these two lines: \n",
    "    \n",
    "<code> export CRDS_PATH=$HOME/crds_cache\n",
    "<code> export CRDS_SERVER_URL=https://jwst-crds.stsci.edu </code>\n",
    "\n",
    "Adding the PATH for the CRDS reference files in the bas_profile is recomended over the <code>!export</code> inline option inside jupyter notebook.\n",
    "\n",
    "Now in your terminal just load the enviroment  <env_name>, execute jupyter notebook and you are done!: \n",
    "\n",
    "<code> conda activate < env_name >\n",
    "<code> jupyter notebook </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb280ddd",
   "metadata": {},
   "source": [
    "**INSTALL INLINE THE REQUIRED VERSION OF THE JWST PIPELINE**\n",
    "\n",
    "!pip install jwst==1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e837016",
   "metadata": {},
   "source": [
    "**Installation of SNOWBLIND**\n",
    "\n",
    "!pip install snowblind\n",
    "\n",
    "**Previously, SNOWBLIND requested me to install separetly SKIMAGE from sci-kit learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2bb0c",
   "metadata": {},
   "source": [
    "#### Please check that the pipeline version is 1.12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jwst\n",
    "print(\"PIPELINE VERSION = \",jwst.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038641f1",
   "metadata": {},
   "source": [
    "**SETUP CRDS_CONTEXT FILE FOR REPROCUDIBILITY**\n",
    "\n",
    "Check list of context files here: https://jwst-crds.stsci.edu/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CRDS_CONTEXT\"] = \"jwst_1256.pmap\"  #### REALEASE DATE 2024-07-26\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6958510",
   "metadata": {},
   "source": [
    "**DEFINE HOW MANY CPUs WILL BE USED FOR THE DATA REDUCTION**\n",
    "\n",
    "To check how nany CPUs are available use <code> os.cpu_count() </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores2use = 3 ##### NUMBER OF CPU CORES TO USE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD NECESARY PACKAGES #####\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import vstack\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### SET WORKING FIRECTORY, WHERE THE RAW DATA IS LOCATED ###########################\n",
    "out_dir = \"/Users/dumont/Documents/ReveaLLGN/RESULTS/M87/\"\n",
    "in_dir = \"/Users/dumont/Documents/ReveaLLGN/DATA/M87/MAST_2024-07-10T1403/JWST/\"\n",
    "\n",
    "saving_path = out_dir + \"DRS3/\" \n",
    "list_folders = sorted(glob(in_dir+\"*\", recursive = True)) # List of folders of the RAW data\n",
    "\n",
    "# Make sure the output directory exists before copying any data\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not os.path.exists(saving_path):\n",
    "    os.makedirs(saving_path)\n",
    "###### COPY FILES TO WORKING DIRECTORY ################\n",
    "print(\"Copping uncal files\")\n",
    "for folder in list_folders: \n",
    "    for uncal_file in glob(folder + \"/*uncal.fits\"):\n",
    "        shutil.copy(uncal_file, out_dir)\n",
    "print(\"Done copping uncal files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fe7d4",
   "metadata": {},
   "source": [
    "# RUN STAGE 1 PIPELINE\n",
    "### with multiprocessing\n",
    "\n",
    "Here we load and run STAGE1 pipeline, saving the JUMP output to flag snoball with an external code called \"snoblind\". The notebook uses the python scrypt **JWST_PIPELINE.py** to run specific steps of the pipeline and also allowing multiprocessing for speeding up the data reduction.\n",
    "\n",
    "<span style=\"color:red\"> IMPORTANT !!! </span> STAGE1 pipeline requires OPENCV installed to Run JUMP detection. Please PIP install OPENCV in your terminal before opening this jupyter notebook.:\n",
    "\n",
    "<code> pip install -q opencv-python </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0226522",
   "metadata": {},
   "source": [
    "### FIRST RUN STAGE 1 PIPELINE ONLY TO SAVE JUMP FILES TO REMOVE SNOWBALLS\n",
    "if you want to skip the snowball flagging, then simply run STAGE1 pipeline normally, i.e remove the jump.expand_large_events = False below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeaec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JWST_PIPELINE import PIPELINE_DETECTOR1_BEFORE_SNOWBLIND,mk_config_det1_before_snowblind\n",
    "\n",
    "# get the uncal files to run\n",
    "uncal_files_list =  glob(out_dir + \"/*_uncal.fits\")\n",
    "## MAKE CONFIG FILE ##\n",
    "mk_config_det1_before_snowblind(out_dir,uncal_files_list[0]) ### CHOOSE RANDOM UNCAL FILE\n",
    "print('Will run the pipeline on {} files'.format(len(uncal_files_list)))\n",
    "# the output list should be the same length as the files to run\n",
    "outptd = [out_dir for _ in range(len(uncal_files_list))]\n",
    "# set the pool and run multiprocess\n",
    "with multiprocessing.Pool(cores2use) as pool:\n",
    "    pool.starmap(PIPELINE_DETECTOR1_BEFORE_SNOWBLIND, zip(uncal_files_list, outptd))\n",
    "    \n",
    "### REMOVE TEMPORARY UNCAL FILES OUTPUT DIRECTORY ################\n",
    "for file in glob(out_dir + \"*_uncal.fits\"):\n",
    "    os.remove(file)  \n",
    "### COPY JUMP TO SAVING PATH #############\n",
    "for jump_file in glob(out_dir + \"*_jump.fits\"):\n",
    "    jump_folder = saving_path + 'JUMPS_B_snowblind/'\n",
    "    if not os.path.exists(jump_folder):\n",
    "        os.makedirs(jump_folder)\n",
    "    shutil.copy(jump_file, jump_folder)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9787a94",
   "metadata": {},
   "source": [
    "## RUN SNOBALL REMOVAL WITH SNOWBLIND\n",
    "\n",
    "> __IMPORTANT CHANGES FROM PREVIOUS PIPELINE VERSION__\n",
    "Previous pipeline routine we used the Chris Willot code _DoSnowballFlag_ for removing large cosmic ray hits. \n",
    "Since then a new code has been developed _Snowblind_ by James Davis at MPIA, and based on my test it performs better. Thus, we use it here instead of _DoSnowballFlag_.\n",
    "\n",
    "Based on the test on Leak Images from M81 and NGC4395 the best parameters are:\n",
    "1. min_radius = 3\n",
    "2. growth_factor = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5c88a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from JWST_PIPELINE import run_snowblind\n",
    "### LOAD ALL JUMP FILES FROM THE OUTPUT DIRECTORY\n",
    "jump_files_list  =  glob(out_dir + \"/*_jump.fits\")\n",
    "# set the pool and run multiprocess\n",
    "# the output list should be the same length as the files to run\n",
    "outptd = [out_dir for _ in range(len(jump_files_list))]\n",
    "with multiprocessing.Pool(cores2use) as pool:\n",
    "    pool.starmap(run_snowblind, zip(outptd,jump_files_list))\n",
    "       \n",
    "#### CHANGE THE SUFFIX OF TEMPORARY SNOWBLIND_JUMPS TO JUMPS\n",
    "for jump_file in glob(out_dir + \"*_snowblind.fits\"):\n",
    "    new_name = os.path.basename(jump_file).replace(\"snowblind\",\"jump\")\n",
    "    os.replace(jump_file, os.path.join(os.path.dirname(jump_file),new_name) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9103de7",
   "metadata": {},
   "source": [
    "## RUN THE REST OF STAGE1 PIPELINE ON JUMP.FITS FILES \n",
    "\n",
    "Since in the previous step we skip two steps after the JUMP detection; RampFitStep and GAINSCALE we need to Run the whole pipeline skipping previous steps until RampFittingStep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94beeb",
   "metadata": {},
   "source": [
    "#### Running DETECTOR1 pipeline starting from RampFittingStep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JWST_PIPELINE import PIPELINE_DETECTOR1_AFTER_SNOWBLIND,mk_config_det1_after_snowblind\n",
    "\n",
    "# get the jump files to run\n",
    "jump_files_list  =  glob(out_dir + \"/*_jump.fits\")\n",
    "## MAKE CONFIG FILE ##\n",
    "mk_config_det1_after_snowblind(out_dir,jump_files_list[0]) ### CHOOSE RANDOM JUMP FILE\n",
    "print('Will run the pipeline on {} files'.format(len(jump_files_list)))\n",
    "# the output list should be the same length as the files to run\n",
    "outptd = [out_dir for _ in range(len(jump_files_list))]\n",
    "# set the pool and run multiprocess\n",
    "with multiprocessing.Pool(cores2use) as pool:\n",
    "    pool.starmap(PIPELINE_DETECTOR1_AFTER_SNOWBLIND, zip(jump_files_list, outptd))\n",
    "    \n",
    "\n",
    "### REMOVE TEMPORARY RATEINTS FILES OUTPUT DIRECTORY ################\n",
    "for file in glob(out_dir + \"*_rateints.fits\"):\n",
    "    os.remove(file)  \n",
    "### REMOVE JUMP FILES FROM OUTPUT DIRECTORY ################\n",
    "for file in glob(out_dir + \"*_jump.fits\"):\n",
    "    os.remove(file)  \n",
    "### COPY RATE TO SAVING PATH #############\n",
    "for rate_file in glob(out_dir + \"*_rate.fits\"):\n",
    "    rate_folder = saving_path + 'RATES/'\n",
    "    if not os.path.exists(rate_folder):\n",
    "        os.makedirs(rate_folder)\n",
    "    shutil.copy(rate_file, rate_folder)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7364660",
   "metadata": {},
   "source": [
    "# RUN STAGE 2 PIPELINE\n",
    "\n",
    "NOW THAT SNOWBALLS HAVE BEEN IDENTIFIED and THERMAL FLUCTUATION REMOVED, WE CAN RUN STAGE 2 PIPELINE ON THE SCIENCE AND IMPRINTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### COPY STAGE2 FILES DIRECTORY ################\n",
    "print(\"Copping json files\")\n",
    "for folder in list_folders:\n",
    "    for json_file in glob(folder.rsplit(\"/\",1)[0] + \"/*_nrs1/*json\") :\n",
    "        shutil.copy(json_file, out_dir)\n",
    "print(\"Done copping json files\")\n",
    "###### COPY STAGE2 FILES DIRECTORY ################\n",
    "print(\"Copping json files\")\n",
    "for folder in list_folders:\n",
    "    for json_file in glob(folder.rsplit(\"/\",1)[0] + \"/*_nrs2/*json\") :\n",
    "        shutil.copy(json_file, out_dir)\n",
    "print(\"Done copping json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90348914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JWST_PIPELINE import PIPELINE_DETECTOR2,mk_config_det2\n",
    "\n",
    "###### LOAD ASSOCIATION FILES\n",
    "asn_files = glob(out_dir+\"/*.json\")\n",
    "## MAKE CONFIG FILE ##\n",
    "mk_config_det2(out_dir,asn_files[0]) ### CHOOSE RANDOM ASSOCIATION FILE\n",
    "print('Will run the pipeline on {} files'.format(len(asn_files)))\n",
    "# the output list should be the same length as the files to run\n",
    "outptd = [out_dir for _ in range(len(asn_files))]\n",
    "# set the pool and run multiprocess\n",
    "with multiprocessing.Pool(cores2use) as pool:\n",
    "    pool.starmap(PIPELINE_DETECTOR2, zip(asn_files, outptd))\n",
    "\n",
    "### COPY CAL TO SAVING PATH #############\n",
    "for cal_file in glob(out_dir + \"*_cal.fits\"):\n",
    "    cal_folder = saving_path + 'CAL/'\n",
    "    if not os.path.exists(cal_folder):\n",
    "        os.makedirs(cal_folder)\n",
    "    shutil.copy(cal_file, cal_folder)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d7d00",
   "metadata": {},
   "source": [
    "# RUN STAGE3 PIPELINE\n",
    "\n",
    "Here I run the STAGE3 pipeline with <span style=\"color:blue\"> OUTLIER_DETECTION = ON </span>, <span style=\"color:blue\"> cube_build.weighting='drizzle' </span> and instrument align cubes <span style=\"color:blue\"> cube_build.coord_system= \"ifualign\"  </span>. To change the type of weighting function for the cube reconstruction go to the JWST_PIPELNE.py to the function <code>mk_config_det3() </code> and change <span style=\"color:blue\"> cube_build.weighting='emsm' </span>, and for sky-align cubes (the default output of the piepline) set <span style=\"color:blue\"> cube_build.coord_system = \"skyalign\"  </span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac15b2",
   "metadata": {},
   "source": [
    "#### Now copy the STAGE3 ASN_JASON FILES\n",
    "PLEASE DELETE STAGE 2 JSON FILES AND COPY STAGE 3 JSON FILES IN THE OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881317a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lets first loop over the JASON files in the working directory and delete them.\n",
    "for file in glob(out_dir + \"*.json\"):\n",
    "    os.remove(file)  \n",
    "###### COPY STAGE2 FILES DIRECTORY ################\n",
    "print(\"Copping json files\")\n",
    "for folder in list_folders:\n",
    "    if \"nirspec\" in folder.rsplit(\"/\",1)[1]:\n",
    "        json_file = glob(folder+\"/*.json\")[0]\n",
    "        shutil.copy(json_file, out_dir)\n",
    "print(\"Done copping json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab626c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD THE calwebb_spec3 pipeline\n",
    "from JWST_PIPELINE import PIPELINE_DETECTOR3,mk_config_det3\n",
    "\n",
    "###### LOAD ASSOCIATION FILES\n",
    "asn_files = glob(out_dir+\"/*.json\") ## ALL JASON FILES IN WORKING DIRECTORY \n",
    "## MAKE CONFIG FILE ##\n",
    "mk_config_det3(out_dir,asn_files[0]) ### CHOOSE RANDOM ASSOCIATION FILE\n",
    "print('Will run the pipeline on {} files'.format(len(asn_files)))\n",
    "# the output list should be the same length as the files to run\n",
    "outptd = [out_dir for _ in range(len(asn_files))]\n",
    "# set the pool and run multiprocess\n",
    "with multiprocessing.Pool(cores2use) as pool:\n",
    "    pool.starmap(PIPELINE_DETECTOR3, zip(asn_files, outptd))\n",
    "\n",
    "\n",
    "### COPY CAL TO SAVING PATH #############\n",
    "for cube in glob(out_dir + \"*_s3d.fits\"):\n",
    "    drizsle_folder = saving_path + 'DRIZZLE/'\n",
    "    if not os.path.exists(drizsle_folder):\n",
    "        os.makedirs(drizsle_folder)\n",
    "    shutil.copy(cube, drizsle_folder)   \n",
    "#############################################################################################\n",
    "### Remove all files from working directory   \n",
    "#### Lets first loop over the JASON files in the working directory and delete them.\n",
    "for file in glob(out_dir + \"*.fits\"):\n",
    "    os.remove(file)  \n",
    "for file in glob(out_dir + \"*.json\"):\n",
    "    os.remove(file)  \n",
    "for file in glob(saving_path + \"*_crf.fits\"):\n",
    "    os.remove(file)        \n",
    "    \n",
    "### COPY LOG FILES TO SAVING PATH #############\n",
    "for log in glob(out_dir + \"/*\"):\n",
    "    log_folder = saving_path + 'LOG/'\n",
    "    if not os.path.exists(log_folder):\n",
    "        os.makedirs(log_folder)\n",
    "    if os.path.isfile(log):\n",
    "        shutil.copy(log, log_folder)    \n",
    "        os.remove(log)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
